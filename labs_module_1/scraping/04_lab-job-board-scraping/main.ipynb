{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Job Board Scraping Lab\n",
    "\n",
    "In this lab you will first see a minimal but fully functional code snippet to scrape the LinkedIn Job Search webpage. You will then work on top of the example code and complete several chanllenges.\n",
    "\n",
    "### Some Resources \n",
    "\n",
    "- [Requests library](http://docs.python-requests.org/en/master/#the-user-guide) documentation \n",
    "- [Beautiful Soup Doc](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)\n",
    "- [Urllib](https://docs.python.org/3/library/urllib.html#module-urllib)\n",
    "- [re lib](https://docs.python.org/3/library/re.html)\n",
    "- [Scrapy](https://scrapy.org/)\n",
    "- [List of HTTP status codes](https://en.wikipedia.org/wiki/List_of_HTTP_status_codes)\n",
    "- [HTML basics](http://www.simplehtmlguide.com/cheatsheet.php)\n",
    "- [CSS basics](https://www.cssbasics.com/#page_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required libraries\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "\"\"\"\n",
    "This function searches job posts from LinkedIn and converts the results into a dataframe.\n",
    "\"\"\"\n",
    "def scrape_linkedin_job_search(keywords):\n",
    "    \n",
    "    # Define the base url to be scraped.\n",
    "    # All uppercase variable name signifies this is a constant and its value should never unchange\n",
    "    BASE_URL = 'https://www.linkedin.com/jobs/search/?'\n",
    "    \n",
    "    # Assemble the full url with parameters\n",
    "    scrape_url = ''.join([BASE_URL, 'keywords=', keywords])\n",
    "\n",
    "    # Create a request to get the data from the server \n",
    "    page = requests.get(scrape_url)\n",
    "    soup = BeautifulSoup(page.text, 'html.parser')\n",
    "\n",
    "    # Create an empty dataframe with the columns consisting of the information you want to capture\n",
    "    columns = ['Title', 'Company', 'Location']\n",
    "    data = pd.DataFrame(columns=columns)\n",
    "\n",
    "    # Retrieve HTML code from the webpage. Parse the HTML into a list of \"cards\".\n",
    "    # Then in each job card, extract the job title, company, and location data.\n",
    "    titles = []\n",
    "    companies = []\n",
    "    locations = []\n",
    "    for card in soup.select(\"div.result-card__contents\"):\n",
    "        title = card.findChild(\"h3\", recursive=False)\n",
    "        company = card.findChild(\"h4\", recursive=False)\n",
    "        location = card.findChild(\"span\", attrs={\"class\": \"job-result-card__location\"}, recursive=True)\n",
    "        titles.append(title.string)\n",
    "        companies.append(company.string)\n",
    "        locations.append(location.string)\n",
    "    \n",
    "    # Inject job titles, companies, and locations into the empty dataframe\n",
    "    zipped = zip(titles, companies, locations)\n",
    "    for z in list(zipped):\n",
    "        data=data.append({'Title' : z[0] , 'Company' : z[1], 'Location': z[2]} , ignore_index=True)\n",
    "    \n",
    "    # Return dataframe\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Company</th>\n",
       "      <th>Location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Master Data Management Architect /Manager</td>\n",
       "      <td>Synergy Business Consulting, Inc.</td>\n",
       "      <td>Teaneck, New Jersey, United States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SEO Manager</td>\n",
       "      <td>Onward Select</td>\n",
       "      <td>Atlanta, Georgia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data Governance Analyst</td>\n",
       "      <td>Momentum Consulting Corporation</td>\n",
       "      <td>Miami, Florida</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data Systems Analyst</td>\n",
       "      <td>Cypress HCM</td>\n",
       "      <td>San Francisco, California</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Data Analyst w/ AWS</td>\n",
       "      <td>Digipulse Technologies Inc.</td>\n",
       "      <td>Greensboro, North Carolina, United States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Rebate &amp; Data Analyst (Contract)</td>\n",
       "      <td>SolomonEdwards</td>\n",
       "      <td>Philadelphia, Pennsylvania</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Tibco Spotfire Data Analyst</td>\n",
       "      <td>GSquared Group</td>\n",
       "      <td>Atlanta, Georgia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Data Modeler</td>\n",
       "      <td>Confiance Tech Solutions</td>\n",
       "      <td>Greater Atlanta Area</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>HEDIS Quality Analyst</td>\n",
       "      <td>HealthTECH Resources, Inc.</td>\n",
       "      <td>New York, New York</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Senior Financial Analyst</td>\n",
       "      <td>Talent 360 Solutions</td>\n",
       "      <td>Greater Atlanta Area</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Manager of Business Intelligence</td>\n",
       "      <td>Beyond Finance, Inc.</td>\n",
       "      <td>Houston, Texas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Manager, Provider Network Development (Healthc...</td>\n",
       "      <td>Remedy Partners</td>\n",
       "      <td>Norwalk, Connecticut, United States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>IFRS9/CECL Implementation</td>\n",
       "      <td>New York Technology Partners</td>\n",
       "      <td>New York, New York, United States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Medical Director Pharmacovigilance</td>\n",
       "      <td>Integrated Resources, Inc ( IRI )</td>\n",
       "      <td>Cambridge, Massachusetts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Sr. Internal Auditor</td>\n",
       "      <td>Interface</td>\n",
       "      <td>1280 W Peachtree St NW, Atlanta, Georgia 30309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Workday Consultant</td>\n",
       "      <td>Amiseq Inc.</td>\n",
       "      <td>Frisco, Texas, United States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Vice President - Counter Party Credit</td>\n",
       "      <td>Madison-Davis, LLC</td>\n",
       "      <td>New York, New York, United States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>WorkFusion</td>\n",
       "      <td>New York City, NY, US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Senior Manager Data Analysis</td>\n",
       "      <td>Jobs @ TheJobNetwork</td>\n",
       "      <td>Deerfield, IL, US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>ipsy</td>\n",
       "      <td>San Mateo, CA, US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Headspace Inc.</td>\n",
       "      <td>Santa Monica, CA, US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Master Data Analyst</td>\n",
       "      <td>Larsen &amp; Toubro Infotech Ltd (LTI)</td>\n",
       "      <td>San Jose, California, United States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Senior Data Analyst - Finance &amp; Platform Analy...</td>\n",
       "      <td>Twitch</td>\n",
       "      <td>San Francisco, CA, US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Procurement Specialist â€“ MRO, Data Analysis</td>\n",
       "      <td>Blue Signal Search</td>\n",
       "      <td>Decatur, IL, US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Operations Data Analyst &amp; Program Manager - 74307</td>\n",
       "      <td>AMD</td>\n",
       "      <td>Santa Clara, CA, US</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Title  \\\n",
       "0           Master Data Management Architect /Manager   \n",
       "1                                         SEO Manager   \n",
       "2                             Data Governance Analyst   \n",
       "3                                Data Systems Analyst   \n",
       "4                                 Data Analyst w/ AWS   \n",
       "5                    Rebate & Data Analyst (Contract)   \n",
       "6                         Tibco Spotfire Data Analyst   \n",
       "7                                        Data Modeler   \n",
       "8                               HEDIS Quality Analyst   \n",
       "9                            Senior Financial Analyst   \n",
       "10                   Manager of Business Intelligence   \n",
       "11  Manager, Provider Network Development (Healthc...   \n",
       "12                          IFRS9/CECL Implementation   \n",
       "13                 Medical Director Pharmacovigilance   \n",
       "14                               Sr. Internal Auditor   \n",
       "15                                 Workday Consultant   \n",
       "16              Vice President - Counter Party Credit   \n",
       "17                                       Data Analyst   \n",
       "18                       Senior Manager Data Analysis   \n",
       "19                                       Data Analyst   \n",
       "20                                       Data Analyst   \n",
       "21                                Master Data Analyst   \n",
       "22  Senior Data Analyst - Finance & Platform Analy...   \n",
       "23        Procurement Specialist â€“ MRO, Data Analysis   \n",
       "24  Operations Data Analyst & Program Manager - 74307   \n",
       "\n",
       "                               Company  \\\n",
       "0    Synergy Business Consulting, Inc.   \n",
       "1                        Onward Select   \n",
       "2      Momentum Consulting Corporation   \n",
       "3                          Cypress HCM   \n",
       "4          Digipulse Technologies Inc.   \n",
       "5                       SolomonEdwards   \n",
       "6                       GSquared Group   \n",
       "7             Confiance Tech Solutions   \n",
       "8           HealthTECH Resources, Inc.   \n",
       "9                 Talent 360 Solutions   \n",
       "10                Beyond Finance, Inc.   \n",
       "11                     Remedy Partners   \n",
       "12        New York Technology Partners   \n",
       "13   Integrated Resources, Inc ( IRI )   \n",
       "14                           Interface   \n",
       "15                         Amiseq Inc.   \n",
       "16                  Madison-Davis, LLC   \n",
       "17                          WorkFusion   \n",
       "18                Jobs @ TheJobNetwork   \n",
       "19                                ipsy   \n",
       "20                      Headspace Inc.   \n",
       "21  Larsen & Toubro Infotech Ltd (LTI)   \n",
       "22                              Twitch   \n",
       "23                  Blue Signal Search   \n",
       "24                                 AMD   \n",
       "\n",
       "                                          Location  \n",
       "0               Teaneck, New Jersey, United States  \n",
       "1                                 Atlanta, Georgia  \n",
       "2                                   Miami, Florida  \n",
       "3                        San Francisco, California  \n",
       "4        Greensboro, North Carolina, United States  \n",
       "5                       Philadelphia, Pennsylvania  \n",
       "6                                 Atlanta, Georgia  \n",
       "7                             Greater Atlanta Area  \n",
       "8                               New York, New York  \n",
       "9                             Greater Atlanta Area  \n",
       "10                                  Houston, Texas  \n",
       "11             Norwalk, Connecticut, United States  \n",
       "12               New York, New York, United States  \n",
       "13                        Cambridge, Massachusetts  \n",
       "14  1280 W Peachtree St NW, Atlanta, Georgia 30309  \n",
       "15                    Frisco, Texas, United States  \n",
       "16               New York, New York, United States  \n",
       "17                           New York City, NY, US  \n",
       "18                               Deerfield, IL, US  \n",
       "19                               San Mateo, CA, US  \n",
       "20                            Santa Monica, CA, US  \n",
       "21             San Jose, California, United States  \n",
       "22                           San Francisco, CA, US  \n",
       "23                                 Decatur, IL, US  \n",
       "24                             Santa Clara, CA, US  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example to call the function\n",
    "\n",
    "results = scrape_linkedin_job_search('data%20analysis')\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge 1\n",
    "\n",
    "The first challenge for you is to update the `scrape_linkedin_job_search` function by adding a new parameter called `num_pages`. This will allow you to search more than 25 jobs with this function. Suggested steps:\n",
    "\n",
    "1. Go to https://www.linkedin.com/jobs/search/?keywords=data%20analysis in your browser.\n",
    "1. Scroll down the left panel and click the page 2 link. Look at how the URL changes and identify the page offset parameter.\n",
    "1. Add `num_pages` as a new param to the `scrape_linkedin_job_search` function. Update the function code so that it uses a \"for\" loop to retrieve several pages of search results.\n",
    "1. Test your new function by scraping 5 pages of the search results.\n",
    "\n",
    "Hint: Prepare for the case where there are less than 5 pages of search results. Your function should be robust enough to **not** trigger errors. Simply skip making additional searches and return all results if the search already reaches the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "\n",
    "\n",
    "\n",
    "# Import the required libraries\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "\"\"\"\n",
    "This function searches job posts from LinkedIn and converts the results into a dataframe.\n",
    "\"\"\"\n",
    "def scrape_linkedin_job_search_25plus(keywords,num_pages):\n",
    "    \n",
    "    # Define the base url to be scraped.\n",
    "    # All uppercase variable name signifies this is a constant and its value should never unchange\n",
    "    BASE_URL = 'https://www.linkedin.com/jobs/search/?'\n",
    "    \n",
    "    \n",
    "    titles = []\n",
    "    companies = []\n",
    "    locations = []\n",
    "    \n",
    "    index_counter = 0\n",
    "    while 25*index_counter < num_pages:\n",
    "         # Assemble the full url with parameters\n",
    "        scrape_url = ''.join([BASE_URL, 'keywords=', keywords, \"&start=\",str(25*index_counter)])\n",
    "        \n",
    "        try:\n",
    "            # Create a request to get the data from the server \n",
    "            page = requests.get(scrape_url)\n",
    "        except:\n",
    "            break\n",
    "    \n",
    "        soup = BeautifulSoup(page.text, 'html.parser')\n",
    "\n",
    "        # Retrieve HTML code from the webpage. Parse the HTML into a list of \"cards\".\n",
    "        # Then in each job card, extract the job title, company, and location data.\n",
    "\n",
    "        for card in soup.select(\"div.result-card__contents\"):\n",
    "            title = card.findChild(\"h3\", recursive=False)\n",
    "            company = card.findChild(\"h4\", recursive=False)\n",
    "            location = card.findChild(\"span\", attrs={\"class\": \"job-result-card__location\"}, recursive=True)\n",
    "            titles.append(title.string)\n",
    "            companies.append(company.string)\n",
    "            locations.append(location.string)\n",
    "            \n",
    "        # make sure that it not goes beyond the first site whenever there are less than 25 entries on the first page\n",
    "        if len(soup.select(\"div.result-card__contents\")) < 25:\n",
    "            break\n",
    "        else:            \n",
    "            index_counter += 1\n",
    "    \n",
    "    # Create an empty dataframe with the columns consisting of the information you want to capture\n",
    "    columns = ['Title', 'Company', 'Location']\n",
    "    data = pd.DataFrame(columns=columns)\n",
    "    \n",
    "    \n",
    "    # Inject job titles, companies, and locations into the empty dataframe\n",
    "    zipped = zip(titles, companies, locations)\n",
    "    for z in list(zipped):\n",
    "        data=data.append({'Title' : z[0] , 'Company' : z[1], 'Location': z[2]} , ignore_index=True)\n",
    "    \n",
    "    # Return dataframe\n",
    "    if len(data) > num_pages:\n",
    "        return data.iloc[0:num_pages]\n",
    "    else:\n",
    "        return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Company</th>\n",
       "      <th>Location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>MakeSpace</td>\n",
       "      <td>New York, New York, United States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>University of Colorado Colorado Springs</td>\n",
       "      <td>Colorado Springs, CO, US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>VP Data Transformation</td>\n",
       "      <td>Centria Healthcare</td>\n",
       "      <td>Farmington Hills, Michigan, United States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>BMW of North America, LLC</td>\n",
       "      <td>Woodcliff Lake, NJ, US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Hollstadt Consulting</td>\n",
       "      <td>Greater Minneapolis-St. Paul Area</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Data Ops Analyst</td>\n",
       "      <td>Wish</td>\n",
       "      <td>San Francisco, CA, US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Quest Financial</td>\n",
       "      <td>Dunwoody, Georgia, United States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Data Strategy Analyst</td>\n",
       "      <td>Payden &amp; Rygel</td>\n",
       "      <td>Los Angeles, California, United States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Associate Data Analyst</td>\n",
       "      <td>ZoomInfo</td>\n",
       "      <td>Vancouver, WA, US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>State of North Carolina</td>\n",
       "      <td>Raleigh, NC, US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Strategic Data Analyst</td>\n",
       "      <td>PDC - A Brady Business</td>\n",
       "      <td>Valencia, California</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>TEKsystems</td>\n",
       "      <td>Summit, New Jersey, United States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Eliassen Group</td>\n",
       "      <td>Tampa, Florida, United States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Data and Business Intelligence Analyst</td>\n",
       "      <td>Times Union</td>\n",
       "      <td>Albany, New York</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Reporting &amp; Metrics Data Analyst</td>\n",
       "      <td>Airbnb</td>\n",
       "      <td>San Francisco, CA, US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Marchon Partners</td>\n",
       "      <td>Charlotte Metro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Analyst</td>\n",
       "      <td>YouGov</td>\n",
       "      <td>Cheshire, Connecticut, United States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Data Analyst, Workforce Management</td>\n",
       "      <td>Sun Country Airlines</td>\n",
       "      <td>Minneapolis, MN, US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Progressive GE</td>\n",
       "      <td>San Diego, California</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Versique Search &amp; Consulting</td>\n",
       "      <td>Hopkins, MN, US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>IPSY</td>\n",
       "      <td>San Mateo, CA, US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Crime and Intelligence Analyst</td>\n",
       "      <td>San Jose Police Department</td>\n",
       "      <td>San Jose, California, United States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Data Analyst, Digital</td>\n",
       "      <td>HAUS LABORATORIES</td>\n",
       "      <td>Los Angeles, California, United States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Data &amp; Reporting Analyst</td>\n",
       "      <td>Astravo</td>\n",
       "      <td>Greater Denver Area</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>EPITEC</td>\n",
       "      <td>Plano, Texas, United States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>MakeSpace</td>\n",
       "      <td>New York, New York, United States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>VP Data Transformation</td>\n",
       "      <td>Centria Healthcare</td>\n",
       "      <td>Farmington Hills, Michigan, United States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>BMW of North America, LLC</td>\n",
       "      <td>Woodcliff Lake, NJ, US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Planted, Inc.</td>\n",
       "      <td>New York City, NY, US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Data Ops Analyst</td>\n",
       "      <td>Wish</td>\n",
       "      <td>San Francisco, CA, US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Quest Financial</td>\n",
       "      <td>Dunwoody, Georgia, United States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Data Strategy Analyst</td>\n",
       "      <td>Payden &amp; Rygel</td>\n",
       "      <td>Los Angeles, California, United States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Associate Data Analyst</td>\n",
       "      <td>ZoomInfo</td>\n",
       "      <td>Vancouver, WA, US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>State of North Carolina</td>\n",
       "      <td>Raleigh, NC, US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Strategic Data Analyst</td>\n",
       "      <td>PDC - A Brady Business</td>\n",
       "      <td>Valencia, California</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>TEKsystems</td>\n",
       "      <td>Summit, New Jersey, United States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Eliassen Group</td>\n",
       "      <td>Tampa, Florida, United States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Data and Business Intelligence Analyst</td>\n",
       "      <td>Times Union</td>\n",
       "      <td>Albany, New York</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Reporting &amp; Metrics Data Analyst</td>\n",
       "      <td>Airbnb</td>\n",
       "      <td>San Francisco, CA, US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>Analyst, Data and Analytics</td>\n",
       "      <td>Digitas North America</td>\n",
       "      <td>Greater New York City Area</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>Analytics - Insights Analyst</td>\n",
       "      <td>Starz</td>\n",
       "      <td>New York City Metropolitan Area</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Marchon Partners</td>\n",
       "      <td>Charlotte Metro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>Data Scientist, Charging Data and Modeling</td>\n",
       "      <td>Tesla</td>\n",
       "      <td>Palo Alto, CA, US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>Analyst</td>\n",
       "      <td>YouGov</td>\n",
       "      <td>Cheshire, Connecticut, United States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Progressive GE</td>\n",
       "      <td>San Diego, California</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Versique Search &amp; Consulting</td>\n",
       "      <td>Hopkins, MN, US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>IPSY</td>\n",
       "      <td>San Mateo, CA, US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Dixon Valve</td>\n",
       "      <td>Chestertown, Maryland, United States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>Data Analyst, Digital</td>\n",
       "      <td>HAUS LABORATORIES</td>\n",
       "      <td>Los Angeles, California, United States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>EPITEC</td>\n",
       "      <td>Plano, Texas, United States</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Title  \\\n",
       "0                                 Data Analyst   \n",
       "1                                 Data Analyst   \n",
       "2                       VP Data Transformation   \n",
       "3                               Data Scientist   \n",
       "4                                 Data Analyst   \n",
       "5                             Data Ops Analyst   \n",
       "6                                 Data Analyst   \n",
       "7                        Data Strategy Analyst   \n",
       "8                       Associate Data Analyst   \n",
       "9                                 Data Analyst   \n",
       "10                      Strategic Data Analyst   \n",
       "11                                Data Analyst   \n",
       "12                                Data Analyst   \n",
       "13      Data and Business Intelligence Analyst   \n",
       "14            Reporting & Metrics Data Analyst   \n",
       "15                                Data Analyst   \n",
       "16                                     Analyst   \n",
       "17          Data Analyst, Workforce Management   \n",
       "18                                Data Analyst   \n",
       "19                                Data Analyst   \n",
       "20                                Data Analyst   \n",
       "21              Crime and Intelligence Analyst   \n",
       "22                       Data Analyst, Digital   \n",
       "23                    Data & Reporting Analyst   \n",
       "24                                Data Analyst   \n",
       "25                                Data Analyst   \n",
       "26                      VP Data Transformation   \n",
       "27                              Data Scientist   \n",
       "28                                Data Analyst   \n",
       "29                            Data Ops Analyst   \n",
       "30                                Data Analyst   \n",
       "31                       Data Strategy Analyst   \n",
       "32                      Associate Data Analyst   \n",
       "33                                Data Analyst   \n",
       "34                      Strategic Data Analyst   \n",
       "35                                Data Analyst   \n",
       "36                                Data Analyst   \n",
       "37      Data and Business Intelligence Analyst   \n",
       "38            Reporting & Metrics Data Analyst   \n",
       "39                 Analyst, Data and Analytics   \n",
       "40                Analytics - Insights Analyst   \n",
       "41                                Data Analyst   \n",
       "42  Data Scientist, Charging Data and Modeling   \n",
       "43                                     Analyst   \n",
       "44                                Data Analyst   \n",
       "45                                Data Analyst   \n",
       "46                                Data Analyst   \n",
       "47                                Data Analyst   \n",
       "48                       Data Analyst, Digital   \n",
       "49                                Data Analyst   \n",
       "\n",
       "                                    Company  \\\n",
       "0                                 MakeSpace   \n",
       "1   University of Colorado Colorado Springs   \n",
       "2                        Centria Healthcare   \n",
       "3                 BMW of North America, LLC   \n",
       "4                      Hollstadt Consulting   \n",
       "5                                      Wish   \n",
       "6                           Quest Financial   \n",
       "7                            Payden & Rygel   \n",
       "8                                  ZoomInfo   \n",
       "9                   State of North Carolina   \n",
       "10                   PDC - A Brady Business   \n",
       "11                               TEKsystems   \n",
       "12                           Eliassen Group   \n",
       "13                              Times Union   \n",
       "14                                   Airbnb   \n",
       "15                         Marchon Partners   \n",
       "16                                   YouGov   \n",
       "17                     Sun Country Airlines   \n",
       "18                           Progressive GE   \n",
       "19             Versique Search & Consulting   \n",
       "20                                     IPSY   \n",
       "21               San Jose Police Department   \n",
       "22                        HAUS LABORATORIES   \n",
       "23                                  Astravo   \n",
       "24                                   EPITEC   \n",
       "25                                MakeSpace   \n",
       "26                       Centria Healthcare   \n",
       "27                BMW of North America, LLC   \n",
       "28                            Planted, Inc.   \n",
       "29                                     Wish   \n",
       "30                          Quest Financial   \n",
       "31                           Payden & Rygel   \n",
       "32                                 ZoomInfo   \n",
       "33                  State of North Carolina   \n",
       "34                   PDC - A Brady Business   \n",
       "35                               TEKsystems   \n",
       "36                           Eliassen Group   \n",
       "37                              Times Union   \n",
       "38                                   Airbnb   \n",
       "39                    Digitas North America   \n",
       "40                                    Starz   \n",
       "41                         Marchon Partners   \n",
       "42                                    Tesla   \n",
       "43                                   YouGov   \n",
       "44                           Progressive GE   \n",
       "45             Versique Search & Consulting   \n",
       "46                                     IPSY   \n",
       "47                              Dixon Valve   \n",
       "48                        HAUS LABORATORIES   \n",
       "49                                   EPITEC   \n",
       "\n",
       "                                     Location  \n",
       "0           New York, New York, United States  \n",
       "1                    Colorado Springs, CO, US  \n",
       "2   Farmington Hills, Michigan, United States  \n",
       "3                      Woodcliff Lake, NJ, US  \n",
       "4           Greater Minneapolis-St. Paul Area  \n",
       "5                       San Francisco, CA, US  \n",
       "6            Dunwoody, Georgia, United States  \n",
       "7      Los Angeles, California, United States  \n",
       "8                           Vancouver, WA, US  \n",
       "9                             Raleigh, NC, US  \n",
       "10                       Valencia, California  \n",
       "11          Summit, New Jersey, United States  \n",
       "12              Tampa, Florida, United States  \n",
       "13                           Albany, New York  \n",
       "14                      San Francisco, CA, US  \n",
       "15                            Charlotte Metro  \n",
       "16       Cheshire, Connecticut, United States  \n",
       "17                        Minneapolis, MN, US  \n",
       "18                      San Diego, California  \n",
       "19                            Hopkins, MN, US  \n",
       "20                          San Mateo, CA, US  \n",
       "21        San Jose, California, United States  \n",
       "22     Los Angeles, California, United States  \n",
       "23                        Greater Denver Area  \n",
       "24                Plano, Texas, United States  \n",
       "25          New York, New York, United States  \n",
       "26  Farmington Hills, Michigan, United States  \n",
       "27                     Woodcliff Lake, NJ, US  \n",
       "28                      New York City, NY, US  \n",
       "29                      San Francisco, CA, US  \n",
       "30           Dunwoody, Georgia, United States  \n",
       "31     Los Angeles, California, United States  \n",
       "32                          Vancouver, WA, US  \n",
       "33                            Raleigh, NC, US  \n",
       "34                       Valencia, California  \n",
       "35          Summit, New Jersey, United States  \n",
       "36              Tampa, Florida, United States  \n",
       "37                           Albany, New York  \n",
       "38                      San Francisco, CA, US  \n",
       "39                 Greater New York City Area  \n",
       "40            New York City Metropolitan Area  \n",
       "41                            Charlotte Metro  \n",
       "42                          Palo Alto, CA, US  \n",
       "43       Cheshire, Connecticut, United States  \n",
       "44                      San Diego, California  \n",
       "45                            Hopkins, MN, US  \n",
       "46                          San Mateo, CA, US  \n",
       "47       Chestertown, Maryland, United States  \n",
       "48     Los Angeles, California, United States  \n",
       "49                Plano, Texas, United States  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scrape_linkedin_job_search_25plus('data%20analysis',50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge 2\n",
    "\n",
    "Further improve your function so that it can search jobs in a specific country. Add the 3rd param to your function called `country`. The steps are identical to those in Challange 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "\n",
    "def scrape_linkedin_job_search_25plus_city(keywords,num_pages,city):\n",
    "    \n",
    "    # Define the base url to be scraped.\n",
    "    # All uppercase variable name signifies this is a constant and its value should never unchange\n",
    "    BASE_URL = 'https://www.linkedin.com/jobs/search/?'\n",
    "    \n",
    "    \n",
    "    titles = []\n",
    "    companies = []\n",
    "    locations = []\n",
    "    \n",
    "    index_counter = 0\n",
    "    while 25*index_counter < num_pages:\n",
    "         # Assemble the full url with parameters\n",
    "        scrape_url = ''.join([BASE_URL, 'keywords=', keywords, \"&start=\",str(25*index_counter),\"&location=\",city])\n",
    "        \n",
    "        try:\n",
    "            # Create a request to get the data from the server \n",
    "            page = requests.get(scrape_url)\n",
    "        except:\n",
    "            break\n",
    "    \n",
    "        soup = BeautifulSoup(page.text, 'html.parser')\n",
    "\n",
    "        # Retrieve HTML code from the webpage. Parse the HTML into a list of \"cards\".\n",
    "        # Then in each job card, extract the job title, company, and location data.\n",
    "\n",
    "        for card in soup.select(\"div.result-card__contents\"):\n",
    "            title = card.findChild(\"h3\", recursive=False)\n",
    "            company = card.findChild(\"h4\", recursive=False)\n",
    "            location = card.findChild(\"span\", attrs={\"class\": \"job-result-card__location\"}, recursive=True)\n",
    "            titles.append(title.string)\n",
    "            companies.append(company.string)\n",
    "            locations.append(location.string)\n",
    "            \n",
    "        # make sure that it not goes beyond the first site whenever there are less than 25 entries on the first page\n",
    "        if len(soup.select(\"div.result-card__contents\")) < 25:\n",
    "            break\n",
    "        else:            \n",
    "            index_counter += 1\n",
    "    \n",
    "    # Create an empty dataframe with the columns consisting of the information you want to capture\n",
    "    columns = ['Title', 'Company', 'Location']\n",
    "    data = pd.DataFrame(columns=columns)\n",
    "    \n",
    "    \n",
    "    # Inject job titles, companies, and locations into the empty dataframe\n",
    "    zipped = zip(titles, companies, locations)\n",
    "    for z in list(zipped):\n",
    "        data=data.append({'Title' : z[0] , 'Company' : z[1], 'Location': z[2]} , ignore_index=True)\n",
    "    \n",
    "    # Return dataframe\n",
    "    if len(data) > num_pages:\n",
    "        return data.iloc[0:num_pages]\n",
    "    else:\n",
    "        return data\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Company</th>\n",
       "      <th>Location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Risk &amp; Finance Analyst</td>\n",
       "      <td>Tesla</td>\n",
       "      <td>Munich, DE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>Munich, DE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Stylight</td>\n",
       "      <td>Munich, DE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>KeenRecruit</td>\n",
       "      <td>Munich, DE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Data Scientist (m/f/d)</td>\n",
       "      <td>Huawei</td>\n",
       "      <td>Munich, DE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Knowledge Analyst - HR / People Strategy</td>\n",
       "      <td>Boston Consulting Group (BCG)</td>\n",
       "      <td>Munich Area, Germany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>BI / Product Data Analyst</td>\n",
       "      <td>TrustYou</td>\n",
       "      <td>Munich, Bavaria, Germany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Business Analyst</td>\n",
       "      <td>CVMC LTD</td>\n",
       "      <td>Munich, Bavaria, Germany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Data Scientist (m/f/d)</td>\n",
       "      <td>FlixBus</td>\n",
       "      <td>Munich, DE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Strategy Implementation Intern (d/f/m)</td>\n",
       "      <td>Airbus</td>\n",
       "      <td>Munich, DE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>CRM Specialist (m/f/d)</td>\n",
       "      <td>Tiffany &amp; Co.</td>\n",
       "      <td>Munich, DE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>HR Analyst &amp; Reporting Expert (m/f/d) in HR Op...</td>\n",
       "      <td>Allianz</td>\n",
       "      <td>Munich Area, Germany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Work Student Finance Systems &amp; Transformation ...</td>\n",
       "      <td>Comm Scope</td>\n",
       "      <td>Ottobrunn, DE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Data Scientist (m/f/x)</td>\n",
       "      <td>Wirecard</td>\n",
       "      <td>Aschheim, DE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>Pixida GmbH</td>\n",
       "      <td>Munich, DE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>TRAINEE INNOVATION MANAGEMENT AND DATA ANALYTICS.</td>\n",
       "      <td>EIGHTYDOTS</td>\n",
       "      <td>Munich, DE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Professional Consultant (f/m/d) SAP Master Dat...</td>\n",
       "      <td>Camelot ITLab</td>\n",
       "      <td>Munich, Bavaria, Germany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Data Scientist, Global Accounts</td>\n",
       "      <td>Amazon Web Services (AWS)</td>\n",
       "      <td>Munich, DE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>(Senior) Ad-Tech Analyst (f/m/d)</td>\n",
       "      <td>mytheresa.com</td>\n",
       "      <td>Munich, DE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Program Manager, Diversity, Equity, and Inclusion</td>\n",
       "      <td>Google</td>\n",
       "      <td>Munich, DE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Global Digital Analyst (f/m/d)</td>\n",
       "      <td>Essity</td>\n",
       "      <td>Munich, DE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Team Lead Analytics &amp; Data Science</td>\n",
       "      <td>Joyn GmbH</td>\n",
       "      <td>Munich, DE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Analytics Consultant : Munich</td>\n",
       "      <td>InterWorks</td>\n",
       "      <td>Munich, DE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Scientist (f/m/d) Drug Product Development for...</td>\n",
       "      <td>Coriolis Pharma</td>\n",
       "      <td>Munich, DE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Manager Global Operational Pricing (m/f/x)</td>\n",
       "      <td>Daiichi Sankyo Europe GmbH</td>\n",
       "      <td>MÃ¼nchen und Umgebung, Deutschland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Data Analyst / Business Analyst (m/w/d)</td>\n",
       "      <td>Institut FÃ¼r Musikalische Bildung</td>\n",
       "      <td>Munich, DE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Risk &amp; Finance Analyst</td>\n",
       "      <td>Tesla</td>\n",
       "      <td>Munich, DE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>Munich, DE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Data Analyst (m/f/d)</td>\n",
       "      <td>SIXT</td>\n",
       "      <td>Munich, DE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Investment Data Analyst (m/w/d)</td>\n",
       "      <td>Allianz</td>\n",
       "      <td>Munich, DE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Stylight</td>\n",
       "      <td>Munich, DE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>KeenRecruit</td>\n",
       "      <td>Munich, DE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Data Analyst (m/w/d)</td>\n",
       "      <td>Abalon AB</td>\n",
       "      <td>Munich, DE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Data Scientist (m/f/d)</td>\n",
       "      <td>Huawei</td>\n",
       "      <td>Munich, DE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Business Analyst (m/f/x) Pricing &amp; Profitability</td>\n",
       "      <td>Wirecard</td>\n",
       "      <td>Aschheim, DE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Marketing Data Analyst (m/w/d)</td>\n",
       "      <td>Scout24 Group</td>\n",
       "      <td>Munich, DE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Data Scientist (w/m/d)</td>\n",
       "      <td>PwC Deutschland</td>\n",
       "      <td>MÃ¼nchen und Umgebung, Deutschland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Data Analyst (m/w/d)</td>\n",
       "      <td>Bertelsmann SE &amp; Co. KGaA</td>\n",
       "      <td>Munich, DE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Data Scientist (Entry Level)</td>\n",
       "      <td>Celonis</td>\n",
       "      <td>MÃ¼nchen, Bayern, Deutschland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>Knowledge Analyst - HR / People Strategy</td>\n",
       "      <td>Boston Consulting Group (BCG)</td>\n",
       "      <td>Munich Area, Germany</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Title  \\\n",
       "0                              Risk & Finance Analyst   \n",
       "1                                        Data Analyst   \n",
       "2                                      Data Scientist   \n",
       "3                                        Data Analyst   \n",
       "4                              Data Scientist (m/f/d)   \n",
       "5            Knowledge Analyst - HR / People Strategy   \n",
       "6                           BI / Product Data Analyst   \n",
       "7                                    Business Analyst   \n",
       "8                              Data Scientist (m/f/d)   \n",
       "9              Strategy Implementation Intern (d/f/m)   \n",
       "10                             CRM Specialist (m/f/d)   \n",
       "11  HR Analyst & Reporting Expert (m/f/d) in HR Op...   \n",
       "12  Work Student Finance Systems & Transformation ...   \n",
       "13                             Data Scientist (m/f/x)   \n",
       "14                                      Data Engineer   \n",
       "15  TRAINEE INNOVATION MANAGEMENT AND DATA ANALYTICS.   \n",
       "16  Professional Consultant (f/m/d) SAP Master Dat...   \n",
       "17                    Data Scientist, Global Accounts   \n",
       "18                   (Senior) Ad-Tech Analyst (f/m/d)   \n",
       "19  Program Manager, Diversity, Equity, and Inclusion   \n",
       "20                     Global Digital Analyst (f/m/d)   \n",
       "21                 Team Lead Analytics & Data Science   \n",
       "22                      Analytics Consultant : Munich   \n",
       "23  Scientist (f/m/d) Drug Product Development for...   \n",
       "24         Manager Global Operational Pricing (m/f/x)   \n",
       "25            Data Analyst / Business Analyst (m/w/d)   \n",
       "26                             Risk & Finance Analyst   \n",
       "27                                       Data Analyst   \n",
       "28                               Data Analyst (m/f/d)   \n",
       "29                    Investment Data Analyst (m/w/d)   \n",
       "30                                     Data Scientist   \n",
       "31                                       Data Analyst   \n",
       "32                               Data Analyst (m/w/d)   \n",
       "33                             Data Scientist (m/f/d)   \n",
       "34   Business Analyst (m/f/x) Pricing & Profitability   \n",
       "35                     Marketing Data Analyst (m/w/d)   \n",
       "36                             Data Scientist (w/m/d)   \n",
       "37                               Data Analyst (m/w/d)   \n",
       "38                       Data Scientist (Entry Level)   \n",
       "39           Knowledge Analyst - HR / People Strategy   \n",
       "\n",
       "                              Company                           Location  \n",
       "0                               Tesla                         Munich, DE  \n",
       "1                              Amazon                         Munich, DE  \n",
       "2                            Stylight                         Munich, DE  \n",
       "3                         KeenRecruit                         Munich, DE  \n",
       "4                              Huawei                         Munich, DE  \n",
       "5       Boston Consulting Group (BCG)               Munich Area, Germany  \n",
       "6                            TrustYou           Munich, Bavaria, Germany  \n",
       "7                            CVMC LTD           Munich, Bavaria, Germany  \n",
       "8                             FlixBus                         Munich, DE  \n",
       "9                              Airbus                         Munich, DE  \n",
       "10                      Tiffany & Co.                         Munich, DE  \n",
       "11                            Allianz               Munich Area, Germany  \n",
       "12                         Comm Scope                      Ottobrunn, DE  \n",
       "13                           Wirecard                       Aschheim, DE  \n",
       "14                        Pixida GmbH                         Munich, DE  \n",
       "15                         EIGHTYDOTS                         Munich, DE  \n",
       "16                      Camelot ITLab           Munich, Bavaria, Germany  \n",
       "17          Amazon Web Services (AWS)                         Munich, DE  \n",
       "18                      mytheresa.com                         Munich, DE  \n",
       "19                             Google                         Munich, DE  \n",
       "20                             Essity                         Munich, DE  \n",
       "21                          Joyn GmbH                         Munich, DE  \n",
       "22                         InterWorks                         Munich, DE  \n",
       "23                    Coriolis Pharma                         Munich, DE  \n",
       "24         Daiichi Sankyo Europe GmbH  MÃ¼nchen und Umgebung, Deutschland  \n",
       "25  Institut FÃ¼r Musikalische Bildung                         Munich, DE  \n",
       "26                              Tesla                         Munich, DE  \n",
       "27                             Amazon                         Munich, DE  \n",
       "28                               SIXT                         Munich, DE  \n",
       "29                            Allianz                         Munich, DE  \n",
       "30                           Stylight                         Munich, DE  \n",
       "31                        KeenRecruit                         Munich, DE  \n",
       "32                          Abalon AB                         Munich, DE  \n",
       "33                             Huawei                         Munich, DE  \n",
       "34                           Wirecard                       Aschheim, DE  \n",
       "35                      Scout24 Group                         Munich, DE  \n",
       "36                    PwC Deutschland  MÃ¼nchen und Umgebung, Deutschland  \n",
       "37          Bertelsmann SE & Co. KGaA                         Munich, DE  \n",
       "38                            Celonis       MÃ¼nchen, Bayern, Deutschland  \n",
       "39      Boston Consulting Group (BCG)               Munich Area, Germany  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scrape_linkedin_job_search_25plus_city('data%20analysis',40,\"munich\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge 3\n",
    "\n",
    "Add the 4th param called `num_days` to your function to allow it to search jobs posted in the past X days. Note that in the LinkedIn job search the searched timespan is specified with the following param:\n",
    "\n",
    "```\n",
    "f_TPR=r259200\n",
    "```\n",
    "\n",
    "The number part in the param value is the number of seconds. 259,200 seconds equal to 3 days. You need to convert `num_days` to number of seconds and supply that info to LinkedIn job search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "def scrape_linkedin_job_search_25plus_city_numDays(keywords,num_pages,city,num_days):\n",
    "    \n",
    "    # Define the base url to be scraped.\n",
    "    # All uppercase variable name signifies this is a constant and its value should never unchange\n",
    "    BASE_URL = 'https://www.linkedin.com/jobs/search/?'\n",
    "    \n",
    "    time_stamp = \"f_TPR=r\" + str(int(86400*num_days)) + \"&\" \n",
    "    \n",
    "    \n",
    "    titles = []\n",
    "    companies = []\n",
    "    locations = []\n",
    "    \n",
    "    index_counter = 0\n",
    "    while 25*index_counter < num_pages:\n",
    "         # Assemble the full url with parameters\n",
    "        scrape_url = ''.join([BASE_URL, time_stamp,'keywords=', keywords, \"&start=\",str(25*index_counter),\"&location=\",city,\"&f_TP=1\"])\n",
    "        \n",
    "        try:\n",
    "            # Create a request to get the data from the server \n",
    "            page = requests.get(scrape_url)\n",
    "        except:\n",
    "            break\n",
    "    \n",
    "        soup = BeautifulSoup(page.text, 'html.parser')\n",
    "        \n",
    "        # Retrieve HTML code from the webpage. Parse the HTML into a list of \"cards\".\n",
    "        # Then in each job card, extract the job title, company, and location data.\n",
    "\n",
    "       \n",
    "    \n",
    "        \n",
    "        \n",
    "        for card in soup.select(\"div.result-card__contents\"):\n",
    "            title = card.findChild(\"h3\", recursive=False)\n",
    "            company = card.findChild(\"h4\", recursive=False)\n",
    "            location = card.findChild(\"span\", attrs={\"class\": \"job-result-card__location\"}, recursive=True)\n",
    "            \n",
    "            titles.append(title.string)\n",
    "            companies.append(company.string)\n",
    "            locations.append(location.string)\n",
    "         \n",
    "        # make sure that it not goes beyond the first site whenever there are less than 25 entries on the first page\n",
    "        index_counter +=1\n",
    "\n",
    "    \n",
    "    # Create an empty dataframe with the columns consisting of the information you want to capture\n",
    "    columns = ['Title', 'Company', 'Location']\n",
    "    data = pd.DataFrame(columns=columns)\n",
    "    \n",
    "    \n",
    "    # Inject job titles, companies, and locations into the empty dataframe\n",
    "    zipped = zip(titles, companies, locations)\n",
    "    for z in list(zipped):\n",
    "        data=data.append({'Title' : z[0] , 'Company' : z[1], 'Location': z[2]} , ignore_index=True)\n",
    "    \n",
    "    # Return dataframe\n",
    "    if len(data) > num_pages:\n",
    "        return data.iloc[0:num_pages]\n",
    "    else:\n",
    "        return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.linkedin.com/jobs/search/?f_TPR=r8640&keywords=data%20analysis&start=0&location=munich&f_TP=1\n",
      "https://www.linkedin.com/jobs/search/?f_TPR=r8640&keywords=data%20analysis&start=25&location=munich&f_TP=1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Company</th>\n",
       "      <th>Location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SENIOR DATA ANALYST (m/f/d)</td>\n",
       "      <td>Harnham</td>\n",
       "      <td>NÃ¼rnberg Area, Germany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Manager Global Operational Pricing (m/f/x)</td>\n",
       "      <td>Daiichi Sankyo Europe GmbH</td>\n",
       "      <td>MÃ¼nchen und Umgebung, Deutschland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Internship Data Science (m/f/d)</td>\n",
       "      <td>Limehome GmbH</td>\n",
       "      <td>MÃ¼nchen, Bayern, Deutschland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Senior Growth Manager (m/f/d)</td>\n",
       "      <td>FlixBus</td>\n",
       "      <td>Munich, DE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Preclinical Development Manager (m/f/d)</td>\n",
       "      <td>Sandoz</td>\n",
       "      <td>Munich, DE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Program Manager - Alexa Preview Team</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>Munich, DE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Sales Operations Manager (m/f/d)</td>\n",
       "      <td>Personio</td>\n",
       "      <td>MÃ¼nchen, Bayern, Deutschland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Senior Statistician</td>\n",
       "      <td>PSI CRO AG</td>\n",
       "      <td>Munich, DE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>BIG DATA ARCHITECT (M/F/D)</td>\n",
       "      <td>Altran</td>\n",
       "      <td>Munich, DE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Senior IT Audit Consultant</td>\n",
       "      <td>IAC</td>\n",
       "      <td>Munich, Bavaria, Germany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Praktikum - BI / Data Analyst (m/w/d) mit Foku...</td>\n",
       "      <td>DataGuard</td>\n",
       "      <td>Munich, DE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Working Student (m/w/d) Group Taxation</td>\n",
       "      <td>Allianz</td>\n",
       "      <td>Munich, DE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Software Application Architect</td>\n",
       "      <td>Dynamic Biosensors</td>\n",
       "      <td>MÃ¼nchen, Bayern, Deutschland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Account Manger - DACH</td>\n",
       "      <td>ClassPass</td>\n",
       "      <td>Munich, Bavaria, Germany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Lead Software Developer (m/f/d)</td>\n",
       "      <td>Clariant</td>\n",
       "      <td>Munich, DE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Solution Architect</td>\n",
       "      <td>Vlocity</td>\n",
       "      <td>Munich, DE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Full Stack Software Engineer (Python/Django/Vu...</td>\n",
       "      <td>Limehome GmbH</td>\n",
       "      <td>MÃ¼nchen, Bayern, Deutschland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Software Developer (m/f/d) Hadoop</td>\n",
       "      <td>Altran</td>\n",
       "      <td>Munich, DE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>SENIOR DATA ANALYST (m/f/d)</td>\n",
       "      <td>Harnham</td>\n",
       "      <td>NÃ¼rnberg Area, Germany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Senior IT Audit Consultant</td>\n",
       "      <td>IAC</td>\n",
       "      <td>Munich, Bavaria, Germany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Senior Statistician</td>\n",
       "      <td>PSI CRO AG</td>\n",
       "      <td>Munich, DE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>BIG DATA ARCHITECT (M/F/D)</td>\n",
       "      <td>Altran</td>\n",
       "      <td>Munich, DE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Internship Data Science (m/f/d)</td>\n",
       "      <td>Limehome GmbH</td>\n",
       "      <td>MÃ¼nchen, Bayern, Deutschland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Software Application Architect</td>\n",
       "      <td>Dynamic Biosensors</td>\n",
       "      <td>MÃ¼nchen, Bayern, Deutschland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Praktikum - BI / Data Analyst (m/w/d) mit Foku...</td>\n",
       "      <td>DataGuard</td>\n",
       "      <td>Munich, DE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Manager Global Operational Pricing (m/f/x)</td>\n",
       "      <td>Daiichi Sankyo Europe GmbH</td>\n",
       "      <td>MÃ¼nchen und Umgebung, Deutschland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Lead Software Developer (m/f/d)</td>\n",
       "      <td>Clariant</td>\n",
       "      <td>Munich, DE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Preclinical Development Manager (m/f/d)</td>\n",
       "      <td>Sandoz</td>\n",
       "      <td>Munich, DE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Program Manager - Alexa Preview Team</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>Munich, DE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Sales Operations Manager (m/f/d)</td>\n",
       "      <td>Personio</td>\n",
       "      <td>MÃ¼nchen, Bayern, Deutschland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Solution Architect</td>\n",
       "      <td>Vlocity</td>\n",
       "      <td>Munich, DE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Working Student (m/w/d) Group Taxation</td>\n",
       "      <td>Allianz</td>\n",
       "      <td>Munich, DE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Senior Growth Manager (m/f/d)</td>\n",
       "      <td>FlixBus</td>\n",
       "      <td>Munich, DE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Account Manger - DACH</td>\n",
       "      <td>ClassPass</td>\n",
       "      <td>Munich, Bavaria, Germany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Full Stack Software Engineer (Python/Django/Vu...</td>\n",
       "      <td>Limehome GmbH</td>\n",
       "      <td>MÃ¼nchen, Bayern, Deutschland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Software Developer (m/f/d) Hadoop</td>\n",
       "      <td>Altran</td>\n",
       "      <td>Munich, DE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Title  \\\n",
       "0                         SENIOR DATA ANALYST (m/f/d)   \n",
       "1          Manager Global Operational Pricing (m/f/x)   \n",
       "2                     Internship Data Science (m/f/d)   \n",
       "3                       Senior Growth Manager (m/f/d)   \n",
       "4             Preclinical Development Manager (m/f/d)   \n",
       "5                Program Manager - Alexa Preview Team   \n",
       "6                    Sales Operations Manager (m/f/d)   \n",
       "7                                 Senior Statistician   \n",
       "8                          BIG DATA ARCHITECT (M/F/D)   \n",
       "9                          Senior IT Audit Consultant   \n",
       "10  Praktikum - BI / Data Analyst (m/w/d) mit Foku...   \n",
       "11             Working Student (m/w/d) Group Taxation   \n",
       "12                     Software Application Architect   \n",
       "13                              Account Manger - DACH   \n",
       "14                    Lead Software Developer (m/f/d)   \n",
       "15                                 Solution Architect   \n",
       "16  Full Stack Software Engineer (Python/Django/Vu...   \n",
       "17                  Software Developer (m/f/d) Hadoop   \n",
       "18                        SENIOR DATA ANALYST (m/f/d)   \n",
       "19                         Senior IT Audit Consultant   \n",
       "20                                Senior Statistician   \n",
       "21                         BIG DATA ARCHITECT (M/F/D)   \n",
       "22                    Internship Data Science (m/f/d)   \n",
       "23                     Software Application Architect   \n",
       "24  Praktikum - BI / Data Analyst (m/w/d) mit Foku...   \n",
       "25         Manager Global Operational Pricing (m/f/x)   \n",
       "26                    Lead Software Developer (m/f/d)   \n",
       "27            Preclinical Development Manager (m/f/d)   \n",
       "28               Program Manager - Alexa Preview Team   \n",
       "29                   Sales Operations Manager (m/f/d)   \n",
       "30                                 Solution Architect   \n",
       "31             Working Student (m/w/d) Group Taxation   \n",
       "32                      Senior Growth Manager (m/f/d)   \n",
       "33                              Account Manger - DACH   \n",
       "34  Full Stack Software Engineer (Python/Django/Vu...   \n",
       "35                  Software Developer (m/f/d) Hadoop   \n",
       "\n",
       "                       Company                           Location  \n",
       "0                      Harnham             NÃ¼rnberg Area, Germany  \n",
       "1   Daiichi Sankyo Europe GmbH  MÃ¼nchen und Umgebung, Deutschland  \n",
       "2                Limehome GmbH       MÃ¼nchen, Bayern, Deutschland  \n",
       "3                      FlixBus                         Munich, DE  \n",
       "4                       Sandoz                         Munich, DE  \n",
       "5                       Amazon                         Munich, DE  \n",
       "6                     Personio       MÃ¼nchen, Bayern, Deutschland  \n",
       "7                   PSI CRO AG                         Munich, DE  \n",
       "8                       Altran                         Munich, DE  \n",
       "9                          IAC           Munich, Bavaria, Germany  \n",
       "10                  DataGuard                          Munich, DE  \n",
       "11                     Allianz                         Munich, DE  \n",
       "12          Dynamic Biosensors       MÃ¼nchen, Bayern, Deutschland  \n",
       "13                   ClassPass           Munich, Bavaria, Germany  \n",
       "14                    Clariant                         Munich, DE  \n",
       "15                     Vlocity                         Munich, DE  \n",
       "16               Limehome GmbH       MÃ¼nchen, Bayern, Deutschland  \n",
       "17                      Altran                         Munich, DE  \n",
       "18                     Harnham             NÃ¼rnberg Area, Germany  \n",
       "19                         IAC           Munich, Bavaria, Germany  \n",
       "20                  PSI CRO AG                         Munich, DE  \n",
       "21                      Altran                         Munich, DE  \n",
       "22               Limehome GmbH       MÃ¼nchen, Bayern, Deutschland  \n",
       "23          Dynamic Biosensors       MÃ¼nchen, Bayern, Deutschland  \n",
       "24                  DataGuard                          Munich, DE  \n",
       "25  Daiichi Sankyo Europe GmbH  MÃ¼nchen und Umgebung, Deutschland  \n",
       "26                    Clariant                         Munich, DE  \n",
       "27                      Sandoz                         Munich, DE  \n",
       "28                      Amazon                         Munich, DE  \n",
       "29                    Personio       MÃ¼nchen, Bayern, Deutschland  \n",
       "30                     Vlocity                         Munich, DE  \n",
       "31                     Allianz                         Munich, DE  \n",
       "32                     FlixBus                         Munich, DE  \n",
       "33                   ClassPass           Munich, Bavaria, Germany  \n",
       "34               Limehome GmbH       MÃ¼nchen, Bayern, Deutschland  \n",
       "35                      Altran                         Munich, DE  "
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scrape_linkedin_job_search_25plus_city_numDays('data%20analysis',50,\"munich\",0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus Challenge\n",
    "\n",
    "Allow your function to also retrieve the \"Seniority Level\" of each job searched. Note that the Seniority Level info is not in the initial search results. You need to make a separate search request for each job card based on the `currentJobId` value which you can extract from the job card HTML.\n",
    "\n",
    "After you obtain the Seniority Level info, update the function and add it to a new column of the returned dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "import numpy as np\n",
    "\n",
    "# your code here\n",
    "def scrape_linkedin_job_search_25plus_city_numDays_level(keywords,num_pages,city,num_days):\n",
    "    \n",
    "    # Define the base url to be scraped.\n",
    "    # All uppercase variable name signifies this is a constant and its value should never unchange\n",
    "    BASE_URL = 'https://www.linkedin.com/jobs/search/?'\n",
    "    \n",
    "    time_stamp = \"f_TPR=r\" + str(int(86400*num_days)) + \"&\" \n",
    "    \n",
    "    \n",
    "    titles = []\n",
    "    companies = []\n",
    "    locations = []\n",
    "    senior_levels = []\n",
    "    date_times = []\n",
    "    \n",
    "    index_counter = 0\n",
    "    while 25*index_counter <= num_pages:\n",
    "         # Assemble the full url with parameters\n",
    "        scrape_url = ''.join([BASE_URL, time_stamp,'keywords=', keywords, \"&start=\",str(25*index_counter),\"&location=\",city,\"&f_TP=1\"])\n",
    "        \n",
    "        try:\n",
    "            # Create a request to get the data from the server \n",
    "            page = requests.get(scrape_url)\n",
    "        except:\n",
    "            break\n",
    "    \n",
    "        soup = BeautifulSoup(page.text, 'html.parser')\n",
    "        \n",
    "        # Retrieve HTML code from the webpage. Parse the HTML into a list of \"cards\".\n",
    "        # Then in each job card, extract the job title, company, and location data.\n",
    "\n",
    "        # list of all li blocks. every block contains one search result\n",
    "        job_cards = soup.find(\"ul\",attrs={\"class\": \"jobs-search__results-list\"}).find_all(\"li\",attrs={\"class\": \"result-card\"})\n",
    "        \n",
    "        for card in job_cards:\n",
    "            \n",
    "            # this is the old div.result-card__contents from the other functions\n",
    "            card_div = card.find_all(\"div\", attrs = {\"class\" : \"result-card__contents\"})[0]\n",
    "                        \n",
    "            title = card_div.findChild(\"h3\", recursive=False)\n",
    "            company = card_div.findChild(\"h4\", recursive=False)\n",
    "            location = card_div.findChild(\"span\", attrs={\"class\": \"job-result-card__location\"}, recursive=True)\n",
    "            \n",
    "            titles.append(title.string)\n",
    "            companies.append(company.string)\n",
    "            locations.append(location.string)\n",
    "            \n",
    "            if len(card.find_all(\"time\", attrs = {\"class\" : \"job-result-card__listdate--new\"}))>0:\n",
    "                date_times.append(card.find_all(\"time\", attrs = {\"class\" : \"job-result-card__listdate--new\"})[0].get(\"datetime\"))\n",
    "            else:\n",
    "                date_times.append(np.nan)\n",
    "            \n",
    "            \n",
    "            # now lets turn to the senior level part. First, we extract the Job-ID\n",
    "            card_id = card.get(\"data-id\") # this is the \n",
    "            \n",
    "            # get new url   \n",
    "            scrape_url_id = \"https://www.linkedin.com/jobs/view/\" + str(card_id)\n",
    "           \n",
    "            \n",
    "            \n",
    "            \n",
    "            # load new url and parse\n",
    "            page_id = requests.get(scrape_url_id)\n",
    "            soup_id = BeautifulSoup(page_id.text, 'html.parser')\n",
    "            \n",
    "            # find the information\n",
    "            senior_level = soup_id.find(\"span\",attrs = {\"class\" : \"job-criteria__text\"}).string\n",
    "            senior_levels.append(senior_level) \n",
    "\n",
    "           \n",
    "        \n",
    "        \n",
    "        # make sure that it not goes beyond the first site whenever there are less than 25 entries on the first page\n",
    "        if len(soup.select(\"div.result-card__contents\")) < 25:\n",
    "            break\n",
    "        else:            \n",
    "            index_counter += 1\n",
    "            \n",
    "    # Create an empty dataframe with the columns consisting of the information you want to capture\n",
    "    columns = ['Title', 'Company', 'Location']\n",
    "    data = pd.DataFrame(columns=columns)\n",
    "    \n",
    "    \n",
    "    # Inject job titles, companies, and locations into the empty dataframe\n",
    "    zipped = zip(titles, companies, locations,senior_levels,date_times)\n",
    "    for z in list(zipped):\n",
    "        data=data.append({'Title' : z[0] , 'Company' : z[1], 'Location': z[2],\"Level\":z[3],\"Published\":z[4]} , ignore_index=True)\n",
    "    \n",
    "    # Return dataframe\n",
    "    if len(data) > num_pages:\n",
    "        return data.iloc[0:num_pages]\n",
    "    else:\n",
    "        return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Company</th>\n",
       "      <th>Location</th>\n",
       "      <th>Level</th>\n",
       "      <th>Published</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Manager Global Operational Pricing (m/f/x)</td>\n",
       "      <td>Daiichi Sankyo Europe GmbH</td>\n",
       "      <td>MÃ¼nchen und Umgebung, Deutschland</td>\n",
       "      <td>Associate</td>\n",
       "      <td>2020-01-21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Senior Growth Manager (m/f/d)</td>\n",
       "      <td>FlixBus</td>\n",
       "      <td>Munich, DE</td>\n",
       "      <td>Mid-Senior level</td>\n",
       "      <td>2020-01-20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Preclinical Development Manager (m/f/d)</td>\n",
       "      <td>Sandoz</td>\n",
       "      <td>Munich, DE</td>\n",
       "      <td>Mid-Senior level</td>\n",
       "      <td>2020-01-20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Program Manager - Alexa Preview Team</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>Munich, DE</td>\n",
       "      <td>Mid-Senior level</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sales Operations Manager (m/f/d)</td>\n",
       "      <td>Personio</td>\n",
       "      <td>MÃ¼nchen, Bayern, Deutschland</td>\n",
       "      <td>Mid-Senior level</td>\n",
       "      <td>2020-01-20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Senior Statistician</td>\n",
       "      <td>PSI CRO AG</td>\n",
       "      <td>Munich, DE</td>\n",
       "      <td>Mid-Senior level</td>\n",
       "      <td>2020-01-21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>BIG DATA ARCHITECT (M/F/D)</td>\n",
       "      <td>Altran</td>\n",
       "      <td>Munich, DE</td>\n",
       "      <td>Associate</td>\n",
       "      <td>2020-01-21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Senior IT Audit Consultant</td>\n",
       "      <td>IAC</td>\n",
       "      <td>Munich, Bavaria, Germany</td>\n",
       "      <td>Mid-Senior level</td>\n",
       "      <td>2020-01-20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Working Student (m/w/d) Group Taxation</td>\n",
       "      <td>Allianz</td>\n",
       "      <td>Munich, DE</td>\n",
       "      <td>Not Applicable</td>\n",
       "      <td>2020-01-21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Account Manger - DACH</td>\n",
       "      <td>ClassPass</td>\n",
       "      <td>Munich, Bavaria, Germany</td>\n",
       "      <td>Associate</td>\n",
       "      <td>2020-01-20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Lead Software Developer (m/f/d)</td>\n",
       "      <td>Clariant</td>\n",
       "      <td>Munich, DE</td>\n",
       "      <td>Associate</td>\n",
       "      <td>2020-01-21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Solution Architect</td>\n",
       "      <td>Vlocity</td>\n",
       "      <td>Munich, DE</td>\n",
       "      <td>Associate</td>\n",
       "      <td>2020-01-20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Software Developer (m/f/d) Hadoop</td>\n",
       "      <td>Altran</td>\n",
       "      <td>Munich, DE</td>\n",
       "      <td>Associate</td>\n",
       "      <td>2020-01-21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Title                     Company  \\\n",
       "0   Manager Global Operational Pricing (m/f/x)  Daiichi Sankyo Europe GmbH   \n",
       "1                Senior Growth Manager (m/f/d)                     FlixBus   \n",
       "2      Preclinical Development Manager (m/f/d)                      Sandoz   \n",
       "3         Program Manager - Alexa Preview Team                      Amazon   \n",
       "4             Sales Operations Manager (m/f/d)                    Personio   \n",
       "5                          Senior Statistician                  PSI CRO AG   \n",
       "6                   BIG DATA ARCHITECT (M/F/D)                      Altran   \n",
       "7                   Senior IT Audit Consultant                         IAC   \n",
       "8       Working Student (m/w/d) Group Taxation                     Allianz   \n",
       "9                        Account Manger - DACH                   ClassPass   \n",
       "10             Lead Software Developer (m/f/d)                    Clariant   \n",
       "11                          Solution Architect                     Vlocity   \n",
       "12           Software Developer (m/f/d) Hadoop                      Altran   \n",
       "\n",
       "                             Location             Level   Published  \n",
       "0   MÃ¼nchen und Umgebung, Deutschland         Associate  2020-01-21  \n",
       "1                          Munich, DE  Mid-Senior level  2020-01-20  \n",
       "2                          Munich, DE  Mid-Senior level  2020-01-20  \n",
       "3                          Munich, DE  Mid-Senior level         NaN  \n",
       "4        MÃ¼nchen, Bayern, Deutschland  Mid-Senior level  2020-01-20  \n",
       "5                          Munich, DE  Mid-Senior level  2020-01-21  \n",
       "6                          Munich, DE         Associate  2020-01-21  \n",
       "7            Munich, Bavaria, Germany  Mid-Senior level  2020-01-20  \n",
       "8                          Munich, DE    Not Applicable  2020-01-21  \n",
       "9            Munich, Bavaria, Germany         Associate  2020-01-20  \n",
       "10                         Munich, DE         Associate  2020-01-21  \n",
       "11                         Munich, DE         Associate  2020-01-20  \n",
       "12                         Munich, DE         Associate  2020-01-21  "
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scrape_linkedin_job_search_25plus_city_numDays_level('data%20analysis',50,\"munich\",3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
