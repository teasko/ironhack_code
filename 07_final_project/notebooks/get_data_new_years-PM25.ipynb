{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get data on new-years 18/19 and 19/20 for several locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import requests\n",
    "import pickle\n",
    "import datetime\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load locations of interest and all boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/relevant_censors_cities.pickle', 'rb') as handle:\n",
    "    cities_sensors = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "boxes = pd.read_pickle(\"data/boxes_18-19_19-20.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get data for relevant boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define newyears times for 17-18,18-19,19-20\n",
    "time_frames = {\"18_19\": [\"2018-12-31T05:00:00Z\",\"2019-01-01T05:00:00Z\"],\n",
    "              \"19_20\": [\"2019-12-31T05:00:00Z\",\"2020-01-01T05:00:00Z\"]\n",
    "             }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_box_sensor_data(box_id: str,sensor_id:str,timeframe:list):\n",
    "    \"\"\"\n",
    "    - takes box_id, sensor_id and dates specified in timeframe\n",
    "    - requests data for that sensor in that timeframe from opensensemap\n",
    "    - return dataframe with values of the sensor for the specified times\n",
    "    \"\"\"\n",
    "    url = (\"https://api.opensensemap.org/boxes/\" \n",
    "           + f\"{box_id}/data/\"\n",
    "           + sensor_id\n",
    "           + f\"?from-date={timeframe[0]}&to-date={timeframe[1]}\"\n",
    "           + \"&download=false&format=json\")\n",
    "    try:\n",
    "        return pd.DataFrame(requests.get(url).json()).drop(columns = \"location\")\n",
    "    except:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transformed_box_sensor_data(box_id:str, sensor_id:str, timeframe:list, all_times:pd.DataFrame):\n",
    "    \"\"\"\n",
    "    get data transformed data for box and sensor\n",
    "    returns dataframe with data\n",
    "    \"\"\"\n",
    "    \n",
    "    sensor_df = get_box_sensor_data(box_id,sensor_id,timeframe)\n",
    "    \n",
    "    if len(sensor_df) > 0:\n",
    "                   \n",
    "        # convert values to float \n",
    "        sensor_df.value = sensor_df.value.astype(float)\n",
    "        \n",
    "        # convert time-column to datetime\n",
    "        sensor_df.createdAt = pd.to_datetime(sensor_df.createdAt)\n",
    "            \n",
    "        sensor_df.createdAt = sensor_df.createdAt.values\n",
    "        sensor_df.columns = [\"value\",\"date_time\"]\n",
    "            \n",
    "        # get data for the 5 minute intervals\n",
    "        sensor_df = sensor_df.assign(date = pd.to_datetime(sensor_df.date_time.dt.date))\n",
    "        sensor_df = sensor_df.assign(hour = pd.to_timedelta(sensor_df.date_time.dt.hour,unit=\"hours\"))\n",
    "        sensor_df = sensor_df.assign(minute = pd.to_timedelta(sensor_df.date_time.dt.minute - (sensor_df.date_time.dt.minute % 5),unit=\"m\"))\n",
    "        sensor_df = sensor_df.assign(date_time_5min = sensor_df.date+sensor_df.hour+sensor_df.minute)\n",
    "        sensor_df = sensor_df.drop(columns = [\"date\",\"hour\",\"minute\",\"date_time\"])\n",
    "            \n",
    "        sensor_df = sensor_df.groupby(by = \"date_time_5min\").mean().reset_index()\n",
    "        sensor_df = sensor_df.rename(columns = {\"date_time_5min\":\"date_time\"})\n",
    "            \n",
    "        \n",
    "        # merge with all times spans to make sure that missing times in the data is accounted for as NAN\n",
    "        sensor_df = all_times.copy().merge(right = sensor_df, on = \"date_time\",how=\"left\")\n",
    "           \n",
    "        \n",
    "        # since times are in UTC, and Germany local time is UTC+1, we have to add one hour\n",
    "        sensor_df[\"date_time\"] = sensor_df[\"date_time\"] + datetime.timedelta(hours=1)\n",
    "            \n",
    "        return  sensor_df\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sensor_id_from_box_id(box_id:str,boxes,phenom=\"PM10\"):\n",
    "    \"\"\"\n",
    "    look up phenomnen sensor id in boxes datafrage for phenom =  PM10 or PM2.5\n",
    "    \"\"\"\n",
    "    return boxes.loc[boxes._id.eq(box_id),phenom].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_city_data_in_dfs(city_data_dict:dict,times2018,times2019):\n",
    "    \"\"\"\n",
    "    take data for cites an all sensors and put them in one data frame for each city\n",
    "    \"\"\"\n",
    "    \n",
    "    city_data_dfs = {}\n",
    "       \n",
    "    for city,city_data in city_data_dict.items():    \n",
    "        city_data_dfs[city] = {\"18_19\": minutes1819,\n",
    "                               \"19_20\": minutes1920}\n",
    "        \n",
    "        for data in city_data[\"18_19\"][\"regular\"]:\n",
    "            if not (data[\"values\"] is None):         \n",
    "                city_data_dfs[city][\"18_19\"] = (city_data_dfs[city][\"18_19\"]\n",
    "                                                .merge(right = data[\"values\"],\n",
    "                                                       on = \"date_time\")\n",
    "                                               ).rename(columns = {\"value\":data[\"box_id\"]})\n",
    "        \n",
    "        if city_data[\"18_19\"][\"zone\"]:\n",
    "            for data in city_data[\"18_19\"][\"zone\"]:\n",
    "                if not (data[\"values\"] is None):\n",
    "                    city_data_dfs[city][\"18_19\"] = (city_data_dfs[city][\"18_19\"]\n",
    "                                                    .merge(right = data[\"values\"],\n",
    "                                                           on = \"date_time\")\n",
    "                                                   ).rename(columns = {\"value\":\"zone\"})\n",
    "        \n",
    "                \n",
    "        for data in city_data[\"19_20\"][\"regular\"]:\n",
    "            if not (data[\"values\"] is None):         \n",
    "                city_data_dfs[city][\"19_20\"] = (city_data_dfs[city][\"19_20\"]\n",
    "                                                .merge(right = data[\"values\"],\n",
    "                                                       on = \"date_time\")\n",
    "                                               ).rename(columns = {\"value\":data[\"box_id\"]})\n",
    "        if city_data[\"19_20\"][\"zone\"]:\n",
    "            for data in city_data[\"19_20\"][\"zone\"]:\n",
    "                if not (data[\"values\"] is None):\n",
    "                    city_data_dfs[city][\"19_20\"] = (city_data_dfs[city][\"19_20\"]\n",
    "                                                    .merge(right = data[\"values\"],\n",
    "                                                           on = \"date_time\")\n",
    "                                                   ).rename(columns = {\"value\":\"zone\"})\n",
    "            \n",
    "    return  city_data_dfs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_agg_2_city_data_dfs(city_data_dfs):\n",
    "    \"\"\"\n",
    "    add aggregates to the df for every city\n",
    "    \"\"\"\n",
    "    dfs = city_data_dfs.copy()\n",
    "    for city,city_data in city_data_dfs.items():\n",
    "        dfs[city][\"18_19\"][\"mean\"] = dfs[city][\"18_19\"].drop(columns=\"date_time\").mean(axis=1)\n",
    "        dfs[city][\"19_20\"][\"mean\"] = dfs[city][\"19_20\"].drop(columns=\"date_time\").mean(axis=1)\n",
    "    return dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### request data for all the PM10 sensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a df with all the relevant times to catch missing data\n",
    "minutes1819 = pd.DataFrame([datetime.timedelta(minutes = 10)]*24*1*6,columns=[\"deltas\"])\n",
    "minutes1819.deltas = minutes1819.deltas.cumsum()\n",
    "minutes1819[\"date_time\"] = minutes1819.deltas + pd.to_datetime(\"2018-12-31-05:00:00\") - datetime.timedelta(minutes = 10)\n",
    "minutes1819 = minutes1819[[\"date_time\"]]\n",
    "\n",
    "minutes1920 = pd.DataFrame([datetime.timedelta(minutes = 10)]*24*1*6,columns=[\"deltas\"])\n",
    "minutes1920.deltas = minutes1920.deltas.cumsum()\n",
    "minutes1920[\"date_time\"] = minutes1920.deltas + pd.to_datetime(\"2019-12-31-05:00:00\") - datetime.timedelta(minutes = 10)\n",
    "minutes1920 = minutes1920[[\"date_time\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Germany"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hamburg\n",
      "Berlin\n",
      "Duesseldorf\n",
      "München\n",
      "Dresden\n",
      "Köln\n",
      "Frankfurt\n",
      "Stuttgart\n",
      "Nuernberg\n",
      "Leipzig\n",
      "Muenster\n",
      "Duisburg\n"
     ]
    }
   ],
   "source": [
    "# get data for boxes in the cities from requests\n",
    "\n",
    "city_data_dicts={}\n",
    "\n",
    "for city, city_data in cities_sensors.items():\n",
    "    city_data_dicts[city] = {\"18_19\": {\"zone\" : [],\n",
    "                                     \"regular\" : []},\n",
    "                           \"19_20\": {\"zone\" : [],\n",
    "                                     \"regular\" : []}}\n",
    "    print(city)\n",
    "    \n",
    "    if city_data[\"zone\"]:\n",
    "        for box_id in city_data[\"zone\"]:\n",
    "            city_data_dicts[city][\"18_19\"][\"zone\"] += [{\"box_id\": box_id,\n",
    "                                                        \"values\":\n",
    "                                                        get_transformed_box_sensor_data(box_id,\n",
    "                                                                                    get_sensor_id_from_box_id(box_id,boxes,\"PM2.5\"),\n",
    "                                                                                    time_frames[\"18_19\"],\n",
    "                                                                                    minutes1819)\n",
    "                                                       }]\n",
    "            city_data_dicts[city][\"19_20\"][\"zone\"] += [{\"box_d\": box_id,\n",
    "                                                        \"values\":\n",
    "                                                        get_transformed_box_sensor_data(box_id,\n",
    "                                                                                    get_sensor_id_from_box_id(box_id,boxes,\"PM2.5\"),\n",
    "                                                                                    time_frames[\"19_20\"],\n",
    "                                                                                    minutes1920)\n",
    "                                                       }]\n",
    "    for box_id in city_data[\"regular\"]:\n",
    "        city_data_dicts[city][\"18_19\"][\"regular\"] += [{\"box_id\":box_id,\n",
    "                                                       \"values\":\n",
    "                                                       get_transformed_box_sensor_data(box_id,\n",
    "                                                                                    get_sensor_id_from_box_id(box_id,boxes,\"PM2.5\"),\n",
    "                                                                                    time_frames[\"18_19\"],\n",
    "                                                                                    minutes1819)}]\n",
    "        city_data_dicts[city][\"19_20\"][\"regular\"] += [{\"box_id\":box_id,\n",
    "                                                       \"values\":\n",
    "                                                       get_transformed_box_sensor_data(box_id,\n",
    "                                                                                    get_sensor_id_from_box_id(box_id,boxes,\"PM2.5\"),\n",
    "                                                                                    time_frames[\"19_20\"],\n",
    "                                                                                    minutes1920)}]\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# putting all measurements for one city in one df\n",
    "city_data_dfs = transform_city_data_in_dfs(city_data_dicts,minutes1819,minutes1920)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure that 2018 and 2019 contain the same sensors\n",
    "for city in city_data_dfs:\n",
    "    sensors1819 = list(city_data_dfs[city][\"18_19\"].drop(columns = \"date_time\").columns)\n",
    "    sensors1920 = list(city_data_dfs[city][\"19_20\"].drop(columns = \"date_time\").columns)\n",
    "    drop1819 = list(set(sensors1819)-set(sensors1920))\n",
    "    drop1920 = list(set(sensors1920)-set(sensors1819))\n",
    "    city_data_dfs[city][\"18_19\"].drop(columns = list(set(sensors1819)-set(sensors1920)),inplace = True)\n",
    "    city_data_dfs[city][\"19_20\"].drop(columns = list(set(sensors1920)-set(sensors1819)),inplace = True)\n",
    "\n",
    "del sensors1819\n",
    "del sensors1920"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clearing NANs\n",
    "\n",
    "# in total we have 288 time points (len(minutes1819))\n",
    "# we throw out those sensors that are missing more than 20%, i.e. more than 50\n",
    "\n",
    "\n",
    "for city in city_data_dfs:\n",
    "    # identify nans\n",
    "    isna18_19 = city_data_dfs[city][\"18_19\"].isna().sum() #18_19\n",
    "    isna19_20 = city_data_dfs[city][\"19_20\"].isna().sum() # 19_20\n",
    "    \n",
    "    # check which columns to drop  \n",
    "    drop_cols = list(isna18_19 [isna18_19 >10].index) + list(isna19_20 [isna19_20 >10].index)\n",
    "    \n",
    "    # drop from 18_19 and 19_20\n",
    "    city_data_dfs[city][\"18_19\"] = city_data_dfs[city][\"18_19\"].drop(columns = drop_cols)\n",
    "    city_data_dfs[city][\"19_20\"] = city_data_dfs[city][\"19_20\"].drop(columns = drop_cols)\n",
    "\n",
    "    \n",
    "\n",
    "del drop_cols\n",
    "del isna18_19\n",
    "del isna19_20\n",
    "del city"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "## add mean to each city df\n",
    "city_data_dfs =add_agg_2_city_data_dfs(city_data_dfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only two sensors remained, kick them out! \n",
    "\n",
    "cities = [city for city in city_data_dfs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### handling nans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#interplation\n",
    "for city in cities:\n",
    "    city_data_dfs[city][\"18_19\"] = city_data_dfs[city][\"18_19\"].interpolate()\n",
    "    city_data_dfs[city][\"19_20\"] = city_data_dfs[city][\"19_20\"].interpolate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/city_data_dfs_PM25.pickle', 'wb') as handle:\n",
    "    pickle.dump(city_data_dfs, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
