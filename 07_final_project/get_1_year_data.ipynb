{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get hourly data for 2019 for single box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import pickle\n",
    "import geopy.distance\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weekly_data(box_id: str,sensor_id:str,week:list,save_mode = False):\n",
    "    \"\"\"\n",
    "    - takes box_id, sensor_id and dates specified in week\n",
    "    - requests data for that sensor in that timeframe from opensensemap\n",
    "    - return dataframe with values of the sensor for the specified times\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1) get data from API\n",
    "    url = (\"https://api.opensensemap.org/boxes/\" \n",
    "           + f\"{box_id}/data/\"\n",
    "           + sensor_id\n",
    "           + f\"?from-date={week[0]}&to-date={week[1]}\"\n",
    "           + \"&download=false&format=json\")\n",
    "    \n",
    "    df = pd.DataFrame(requests.get(url).json()) \n",
    "    \n",
    "    if len(df) > 0:\n",
    "        if save_mode:\n",
    "            df.to_pickle(f\"data/one_year_berlin/{box_id}_{week[0][:10]}.pkl\")\n",
    "        return pd.DataFrame(requests.get(url).json()).drop(columns = \"location\")\n",
    "    else:\n",
    "        if save_mode:\n",
    "            df.to_pickle(f\"data/one_year_berlin/{box_id}_{week[0][:10]}.pkl\")\n",
    "        return df\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_2_hourly_data(df: pd.DataFrame,week:list):\n",
    "    \"\"\"\n",
    "    imput: dataframe with sensor data\n",
    "    return: dataframe with hourly aggregated data\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "        \n",
    "    # (1) define hourly time-stamps for the week\n",
    "    hours = pd.DataFrame([datetime.timedelta(hours=1)]*24*7,columns=[\"deltas\"])\n",
    "    hours.deltas = hours.deltas.cumsum()\n",
    "    hours[\"date_time\"] = hours.deltas + pd.to_datetime(f\"{week[0][:10]} 00:00:00\") - datetime.timedelta(hours = 1)\n",
    "    hours = hours[[\"date_time\"]]\n",
    "    \n",
    "    # (2) check if df is empty\n",
    "    if len(df) == 0:\n",
    "        return hours.merge(right=pd.DataFrame(columns = [\"date_time\",\"value\"]), on = \"date_time\", how=\"left\")\n",
    "    else:   \n",
    "        df_ret = df.copy()\n",
    "        # convert values to float \n",
    "        df_ret.value = df_ret.value.astype(float)\n",
    "        \n",
    "        # convert time-column to datetime\n",
    "        df_ret.createdAt = pd.to_datetime(df_ret.createdAt)\n",
    "        \n",
    "        df_ret[\"hour\"] = df_ret.createdAt.dt.floor(\"h\")\n",
    "        df_ret = df_ret.drop(columns = \"createdAt\").groupby(by = \"hour\").mean()\n",
    "        df_ret = df_ret.reset_index()\n",
    "        df_ret.columns = [\"date_time\",\"value\"]\n",
    "        df_ret[\"date_time\"] = df_ret[\"date_time\"].values\n",
    "        \n",
    "    \n",
    "        \n",
    "        return hours.merge(right=df_ret,on=\"date_time\",how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_4_sensor(box_id: str, sensor_id:str,dates):\n",
    "    \"\"\"\n",
    "    for the times specified in dates, returns a df with hourly data for all the weeks\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame(columns = [\"date_time\",\"value\"])\n",
    "    for date in dates:\n",
    "        print(date)\n",
    "        df_dates = get_weekly_data(box_id,sensor_id,date)\n",
    "        #df_dates = transform_2_10min_data(df_dates,date)\n",
    "        df_dates = transform_2_hourly_data(df_dates,date)\n",
    "        df = pd.concat([df,df_dates],axis=0,sort=False)\n",
    "    return df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get data for one of the sensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the start and end of all weeks between Monday 2018-12-31 and Sunday 2020-01-05\n",
    "weeks2019 = [[ (datetime.date(2018, 12, 31)+datetime.timedelta(week_num*7)).isoformat()+\"T00:00:00Z\",\n",
    "          (datetime.date(2019, 1, 6)+datetime.timedelta(week_num*7)).isoformat()+\"T23:59:59Z\"\n",
    "         ]\n",
    "         for week_num in range(53)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all boxes\n",
    "boxes = pd.read_pickle(\"data/boxes_18-19_19-20.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only consider boxes in Berlin\n",
    "boxes_berlin = boxes.loc[boxes.City.eq(\"Berlin\"),[\"_id\",\"PM10\",\"coordinates\",\"District\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define reference coordinates (city center)\n",
    "ref_coord = boxes_berlin.loc[boxes_berlin._id.eq(\"592ca4b851d3460011ea2635\"),\"coordinates\"].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate distance to reference\n",
    "boxes_berlin[\"dist\"] = boxes_berlin[\"coordinates\"].apply(lambda x: geopy.distance.distance((ref_coord[1],ref_coord[0]), (x[1],x[0])).km)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define sensors in the center of berlin\n",
    "boxes_berlin_center = boxes_berlin.loc[boxes_berlin[\"dist\"].le(8),].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "boxes_berlin_center_values = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in boxes_berlin_center.index:\n",
    "    # get data from API for box in Mitte\n",
    "    print(i)\n",
    "    if len(boxes_berlin_center_values) == 0:\n",
    "        boxes_berlin_center_values = get_data_4_sensor(boxes_berlin_center.loc[i,\"_id\"],\n",
    "                                                           boxes_berlin_center.loc[i,\"PM10\"],\n",
    "                                                           weeks2019)\n",
    "        \n",
    "    else:\n",
    "        boxes_berlin_center_values[boxes_berlin_center.loc[i,\"_id\"]] = get_data_4_sensor(boxes_berlin_center.loc[i,\"_id\"],\n",
    "                                                                       boxes_berlin_center.loc[i,\"PM10\"],\n",
    "                                                                       weeks2019)[\"value\"]\n",
    "    \n",
    "    clear_output()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename first sensor columns\n",
    "boxes_berlin_center_values = boxes_berlin_center_values.rename(columns = {\"value\":boxes_berlin_center.loc[0,\"_id\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see how many nans are there\n",
    "boxes_nans = boxes_berlin_center_values.isna().sum()/len(boxes_berlin_center_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only consider boxes with less than 10% nans\n",
    "boxes_nans = boxes_nans[boxes_nans.le(0.2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save as pickle\n",
    "boxes_berlin_center_values.to_pickle(\"data/data_berlin_2019_all.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select only those sensors with less than 10% missing\n",
    "boxes_berlin_center_values = boxes_berlin_center_values[list(boxes_nans.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save as pickle\n",
    "boxes_berlin_center_values.to_pickle(\"data/data_berlin_2019.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
